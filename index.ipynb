{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load modules and data\n",
    "\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy import stats\n",
    "import pdb\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I implement basic fine-mapping methods. Firstly I implement a basic method which calculated Bayes likelihood factors given sets of SNPs with their p-values, with the LD structure calculated from 1000 genomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single trait Bayes factor computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To start, we assume that the trait $y$ is modelled as:\n",
    "\n",
    "$$ y = X\\beta + \\epsilon $$\n",
    "\n",
    "Where $X$ is an $n$x$m$ matrix of values consisting of 0,1,2 denoting whether a SNP is homozygous to the common allele, heterozygous, or homozygous to the rare allele respectively. $n$ denotes the number of samples, and $m$ the number of causitive SNPs.\n",
    "\n",
    "We also scale $X$ such that $\\frac{1}{n}\\sum^{n}_{i=1} X_{ij} = 1$, and $\\frac{1}{n}\\sum^{n}_{i=1} X^2_{ij} = 1$ for $j = 1,2, ... m$.\n",
    "\n",
    "We assume $\\epsilon$ ~ $N(0, \\frac{1}{\\tau} I_n)$. We also assume $\\beta$ has a prior normal distribution $N(0,\\nu \\frac{1}{\\tau})$. $\\nu$ is diagonal, $\\beta$ and $\\epsilon$ are independent and we assume all SNPs have the same prior variance $\\sigma^2 \\frac{1}{\\tau}$. Therefore $\\nu = \\sigma^2 I_m$.\n",
    "\n",
    "Now given this prior on $\\beta$, and using $X$ and $\\epsilon$, we can deduce the expectation and mean of $y$.\n",
    "\n",
    "$$E(y \\: | \\: \\tau, X) = E(E(y \\: | \\: \\tau,X,\\beta)) = E(X \\beta) = 0$$ \n",
    "\n",
    "<sub>[ *since* $E(\\beta) = 0$ ]</sub>\n",
    "\n",
    "$$ Var(y \\: | \\: \\tau, X) = E(Var(y \\: | \\tau, X, \\beta)) + Var(E(y \\: | \\: \\tau, X, \\beta)) $$\n",
    "\n",
    "<sub>[ *since* $Var(X \\: | \\: Y) = E(Var(X \\: | \\: Y)) + Var(E(X \\: | \\: Y))$ ]</sub>\n",
    "\n",
    "$$ = E(\\frac{1}{\\tau}I_n) + Var(X \\beta)$$\n",
    "\n",
    "$$ = \\frac{1}{\\tau}( I_n + X \\nu X^T)$$\n",
    "\n",
    "Now, since y is a linear transformation of a multivariate normal random vector,\n",
    "\n",
    "$$ y \\:|\\: \\tau, X \\sim N \\left( 0,\\frac{1}{\\tau}( I_n + X \\nu X^T)) \\right) $$\n",
    "\n",
    "The null distribution is when $\\beta = 0$. In which case,\n",
    "\n",
    "$$y \\:|\\: \\tau, X \\sim N \\left( 0,\\frac{1}{\\tau}I_n \\right) $$\n",
    "\n",
    "Now consider a new variable $z = \\sqrt{\\frac{\\tau}{n}} X^{T}y$:\n",
    "\n",
    "$$ z ~ \\sim N \\left( 0, \\frac{X^T}{n}(I_n + X \\nu X^T) X \\right)$$\n",
    "\n",
    "$$ = N \\left( 0, \\left(\\frac{X^TX}{n} + \\frac{X^TX \\nu X^TX}{n}\\right) \\right)$$\n",
    "\n",
    "Now let $\\Sigma_x = \\frac{X^T X}{n}$. Since all column in $X$ are standardised, this is equivalent to the correlation matrix or, more importantantly, the linkage disequilibirum structure of the SNPs which can be derived from the 1000 genomes data.\n",
    "\n",
    "Then we have:\n",
    "\n",
    "$$ z \\sim N(0, \\Sigma_x + \\Sigma_x n\\nu \\Sigma_x) $$\n",
    "\n",
    "The *Bayes Factor* is the ratio of the likelihood functions under the alternative hypothesis, and under the null hypothesis. It is equivalent to the likelihood ratio.\n",
    "\n",
    "$P_1(z \\:|\\: \\tau, X)$, the likelihood of $z$ under our alternate hypothesis, i.e. when $\\nu \\neq 0$ is:\n",
    "\n",
    "$$ P_1(z \\:|\\: \\tau, X) = 2\\pi^{-\\frac{n}{2}} | \\Sigma_x + \\Sigma_x n\\nu \\Sigma_x |^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x + \\Sigma_x n\\nu \\Sigma_x)^{-1}z\\right)$$\n",
    "\n",
    "$P_0(z \\:| \\: \\tau, X)$, the likelihood of $z$ under the null hypothesis when $\\nu = 0$ is:\n",
    "\n",
    "$$P_0(z \\:| \\: \\tau, X) = 2\\pi^{-\\frac{n}{2}} |\\Sigma_x|^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x)^{-1}z\\right)$$\n",
    "\n",
    "Therefore we calculate the Bayes Factor as:\n",
    "\n",
    "$$ \\frac{\n",
    "| \\Sigma_x + n\\nu \\Sigma_x^2 |^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x + \\Sigma_x n\\nu \\Sigma_x)^{-1}z\\right)\n",
    "}{\n",
    "|\\Sigma_x|^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x)^{-1}z\\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "We assume that $X$ has full column rank, and that $\\Sigma_x$ also has rank $m$ and is non-singular. That is to say, we assume that no two snps are in full linkage disequilibrium.\n",
    "\n",
    "Using the Woodberry matrix identity:\n",
    "\n",
    "$$\n",
    "(\\Sigma_x + \\Sigma_x n\\nu \\Sigma_x)^{-1} = \\Sigma_x^{-1} - ((n\\nu)^{-1} + \\Sigma_x)^{-1}\n",
    "$$\n",
    "\n",
    "Therefore the resulting Bayes Factor is:\n",
    "\n",
    "$$\n",
    "|I_m + n\\nu \\Sigma_x|^\\frac{1}{2} \\exp(\\frac{1}{2}z^T((n\\nu)^{-1} + \\Sigma_x)^{-1}z)\n",
    "$$\n",
    "\n",
    "Crucially, this only depends on inverting matrices of size m, our candidate gene set. Therefore we compute these Bayes Factors using sets of candidate SNPs of size m, and choose the set with the highest calculated Bayes Factor.\n",
    "\n",
    "In practice, we recieve $\\beta$, $se(\\beta)$, and the SNP linkage disequilibrium structure $\\Sigma_x$.\n",
    "\n",
    "Since $X$ is normalised, $\\beta = X^T y$.\n",
    "\n",
    "Also, \n",
    "$$\\tau = \\frac{1}{se(\\beta))}$$\n",
    "\n",
    "Therefore we generate the $z$ vector by calculating:\n",
    "\n",
    "$$\n",
    "z = \\sqrt{\\frac{\\tau}{n}} X^{T}y = \\frac{\\beta}{\\sqrt{n \\times se(\\beta)}}\n",
    "$$\n",
    "\n",
    "The Bayes Factor can then be directly calculated using $z$ and $\\Sigma_x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_tss_1=np.load('summary_stats_g1_tss60.npy')[0]\n",
    "s_tss_2=np.load('summary_stats_g2_tss60.npy')[0]\n",
    "LD_tss_1=np.load('LD_g1_TSS60.npy')\n",
    "LD_tss_2=np.load('LD_g2_TSS60.npy')\n",
    "\n",
    "### Generate z arrays\n",
    "\n",
    "n1 = 10000\n",
    "n2 = 1000\n",
    "z1 = np.array(np.divide(s_tss_1['beta'],np.sqrt(10*s_tss_1['var_beta'])))\n",
    "z2 = np.array(np.divide(s_tss_2['beta'],np.sqrt(n2*s_tss_1['var_beta'])))\n",
    "z1 = np.ndarray.flatten(z1)\n",
    "z2 = np.ndarray.flatten(z2)\n",
    "\n",
    "### Create selection of SNPs\n",
    "def select_snps(z, subset):\n",
    "    return [z[i] for i in subset]\n",
    "\n",
    "#example\n",
    "# for subset in it.combinations(range(len(z1)),3):\n",
    "#     print subset, select_snps(z1, subset)    \n",
    "\n",
    "\n",
    "\n",
    "### Select covariance submatrix\n",
    "\n",
    "def select_cov(cov, subset):\n",
    "    return cov[np.ix_(subset,subset)]\n",
    "\n",
    "#example   \n",
    "#select_cov(LD_tss_1, (0,1,5))\n",
    "\n",
    "### Calculate Bayes Factor\n",
    "\n",
    "def calc_BF(z, cov, v,n):\n",
    "    z = np.matrix(z)\n",
    "    z = z.T\n",
    "    v = np.matrix(v)\n",
    "    coeff = 1. / math.sqrt(np.linalg.det((np.matrix(np.eye(len(z))) + n * np.matrix(v) * np.matrix(cov))))\n",
    "    exponent = 0.5* z.T * np.matrix(((n*v).I + cov)).I * z\n",
    "    return np.array(math.log(coeff) + exponent)[0][0]\n",
    "\n",
    "# example\n",
    "# subset = (0,1,5,8)\n",
    "# cov = select_cov(LD_tss_1, subset)\n",
    "# z = select_snps(z1, subset)\n",
    "# v = np.eye(len(z))/1000\n",
    "# n = 1000\n",
    "# calc_BF(z,cov,v,n)\n",
    "\n",
    "def choose_variant_set(data,k,v):\n",
    "    results = []\n",
    "    for i in range(1,k):\n",
    "        for subset in it.combinations(range(len(data[0])),i):\n",
    "            z = select_snps(data[0], subset)\n",
    "            cov = select_cov(data[1],subset)\n",
    "            n = data[2]\n",
    "            v_matrix = np.eye(len(z)) * v\n",
    "            results.append((subset, calc_BF(z, cov,v_matrix,n)))\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[0:10]\n",
    "\n",
    "k=5\n",
    "data1 = (z1, LD_tss_1,10000)\n",
    "data2 = (z2, LD_tss_2,1000)\n",
    "v = 0.01\n",
    "    \n",
    "set1 = choose_variant_set(data1,k,v)\n",
    "set2 = choose_variant_set(data2,k,v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trait simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given genotype data and an LD structure, simulate a trait which is linearly associated with a variant, or a set of variants. Here I generate a large mxn matrix (m=number of samples, n=number of SNPs), with 0,1,2 as elements.\n",
    "\n",
    "Then, I can choose a set of SNPs, and from these SNPs I generate a trait with a linear model with a given parameter beta, as well as an unexplained variance parameter epsilon.\n",
    "\n",
    "Following this, I try to recover these sets of SNPs. I generate p-values for each SNP being associated with the trait, by individually building univariate linear models for each SNPs, as I understand summary statistics are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Sample genotypes\n",
    "\n",
    "def simulate_genotype(n,m,geno_dist):\n",
    "    X=np.zeros([n,m])\n",
    "    for i in range(m):\n",
    "        X[:,i] = [np.random.choice(a=[0,1,2],p=geno_dist) for x in range(n)]\n",
    "    return np.array(X)\n",
    "\n",
    "###example\n",
    "# X = simulate_genotype(n=10000,m=30,geno_dist=[0.85,0.1,0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simulate_traits(X,eps,snp_group):\n",
    "    \"\"\"\n",
    "    SNPs in the form e.g. {3: 0.9, 5:0.4, 8:0.5}. Dictionary values are the linear model coefficients (beta values).\n",
    "    eps is the level of unexplained variance. X is the genotype information.\n",
    "    \"\"\"\n",
    "    beta = np.array(snp_group.values()).T\n",
    "    snps = snp_group.keys()\n",
    "    eps_vector = np.array(np.random.normal(0,eps,X.shape[0])).T\n",
    "    return np.add(np.dot(X[:,snps], beta), eps_vector)\n",
    "    \n",
    "###examples\n",
    "# y = simulate_traits(X,eps=0.5,snp_group={3: 5, 9: 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_linear_models(X,y):\n",
    "    return [stats.linregress(X[:,i],y) for i in range(X.shape[1])]\n",
    "\n",
    "#example\n",
    "# [x for x in build_linear_models(X,y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### simulate genotypes\n",
    "X = simulate_genotype(n=10000,m=30,geno_dist=[0.85,0.1,0.05])\n",
    "### scale columns\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "### simulate traits\n",
    "y = simulate_traits(X,eps=0.5,snp_group={3: 5, 9: 3})\n",
    "\n",
    "t_statistics = build_linear_models(X,y)\n",
    "\n",
    "beta = [x.slope for x in t_statistics]\n",
    "se_beta = [x.stderr for x in t_statistics]\n",
    "\n",
    "###calcuate z\n",
    "\n",
    "z = np.multiply(np.dot(np.dot(X.T, X),beta), [1/math.sqrt(x) for x in np.dot(10000, se_beta)])\n",
    "\n",
    "data = (z, LD_tss_1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gene_sets = choose_variant_set(data,k=3,v=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 8), 12218122.580318745),\n",
       " ((3, 9), 10794518.344693422),\n",
       " ((3, 4), 9502093.2812252808),\n",
       " ((2, 3), 6633215.1458299421),\n",
       " ((3, 7), 6619117.3670442216),\n",
       " ((1, 3), 4634486.7498053508),\n",
       " ((0, 3), 4457043.7950010402),\n",
       " ((3, 6), 4378606.3434041664),\n",
       " ((3, 10), 4377756.7924876437),\n",
       " ((3, 26), 4201143.6089289887)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.multiply(np.dot(np.dot(X.T, X),beta), [1/math.sqrt(x) for x in np.dot(10000, se_beta)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00277309e-04,  -4.62578229e-07,  -1.08643031e-06,\n",
       "          2.24024765e-07,  -1.40929082e-06],\n",
       "       [ -4.62578229e-07,   1.00379952e-04,  -3.94730670e-07,\n",
       "         -8.25074204e-07,   1.55094601e-06],\n",
       "       [ -1.08643031e-06,  -3.94730670e-07,   1.00234330e-04,\n",
       "          1.80616973e-07,  -1.22208207e-07],\n",
       "       [  2.24024765e-07,  -8.25074204e-07,   1.80616973e-07,\n",
       "          1.00341607e-04,   2.45702831e-08],\n",
       "       [ -1.40929082e-06,   1.55094601e-06,  -1.22208207e-07,\n",
       "          2.45702831e-08,   1.00416936e-04]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(np.dot(X.T, X))[0:5,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LinregressResult(slope=-0.14182115279849819, intercept=1.5960789136574118, rvalue=-0.024398692221444626, pvalue=0.014690213568169715, stderr=0.058115046680547608),\n",
       " LinregressResult(slope=0.08042585229339301, intercept=1.5506255950057626, rvalue=0.013756891569237718, pvalue=0.16895099805245753, stderr=0.058462541613996687),\n",
       " LinregressResult(slope=-0.054337465865640068, intercept=1.5779288753411143, rvalue=-0.0092164157795728615, pvalue=0.35676546979547619, stderr=0.058960656367995273),\n",
       " LinregressResult(slope=4.9799806750174893, intercept=0.56513274032715999, rvalue=0.8544209229523072, pvalue=0.0, stderr=0.03028676282935994),\n",
       " LinregressResult(slope=-0.12314237024992686, intercept=1.5911668712875144, rvalue=-0.020874573645035147, pvalue=0.036849838318216387, stderr=0.058984606493594147),\n",
       " LinregressResult(slope=0.02198781024729992, intercept=1.5627666571788865, rvalue=0.003721168796063505, pvalue=0.70983950321246747, stderr=0.059093952233592394),\n",
       " LinregressResult(slope=0.065844515591474348, intercept=1.5537976755396419, rvalue=0.011317009669791381, pvalue=0.25780443607990589, stderr=0.058183995496035409),\n",
       " LinregressResult(slope=0.0006707506433050052, intercept=1.5669722447384973, rvalue=0.00011414139133412379, pvalue=0.99089417071488384, stderr=0.058770768099261066),\n",
       " LinregressResult(slope=0.055970526047042383, intercept=1.5563417199818326, rvalue=0.0093154973499804533, pvalue=0.35161903114342652, stderr=0.060086641670279363),\n",
       " LinregressResult(slope=2.9853189386125747, intercept=1.0019839770613184, rvalue=0.48916401331079651, pvalue=0.0, stderr=0.053234316074935353),\n",
       " LinregressResult(slope=-0.051340477187520085, intercept=1.5773729475781828, rvalue=-0.0087296255759151432, pvalue=0.38273390650265648, stderr=0.058815415495814913),\n",
       " LinregressResult(slope=-0.0014472317758562725, intercept=1.5673873517833259, rvalue=-0.00024192177442074848, pvalue=0.98070172977380143, stderr=0.059828284653451774),\n",
       " LinregressResult(slope=0.093451904179397874, intercept=1.5479565569743201, rvalue=0.016069898817238755, pvalue=0.10807789779602683, stderr=0.058151692964406143),\n",
       " LinregressResult(slope=0.054564424140693096, intercept=1.5562465317366807, rvalue=0.0092991904057297525, pvalue=0.35246278945863019, stderr=0.058679862983993678),\n",
       " LinregressResult(slope=0.1025191771508783, intercept=1.5464369860270617, rvalue=0.017396732157309365, pvalue=0.081931497378720047, stderr=0.058927109660426025),\n",
       " LinregressResult(slope=-0.10418946142272512, intercept=1.5884532727861951, rvalue=-0.018003759913938624, pvalue=0.0718137809680605, stderr=0.057867353057146201),\n",
       " LinregressResult(slope=0.019311425938210343, intercept=1.5634279566420435, rvalue=0.0032052363243289589, pvalue=0.74860116349057915, stderr=0.06025533286079518),\n",
       " LinregressResult(slope=-0.067712786571648476, intercept=1.5804645849312651, rvalue=-0.01150400253774936, pvalue=0.25002259314852665, stderr=0.058862189418203613),\n",
       " LinregressResult(slope=0.064610859409161725, intercept=1.5541826802588463, rvalue=0.011103812494528534, pvalue=0.26687982398039284, stderr=0.058190227805444056),\n",
       " LinregressResult(slope=-0.065390298934857469, intercept=1.580581792751153, rvalue=-0.011322612115767043, pvalue=0.25756887228850178, stderr=0.057754028977505041),\n",
       " LinregressResult(slope=0.095578422285995848, intercept=1.547931820630108, rvalue=0.016350105073590068, pvalue=0.10206678524030975, stderr=0.058455406680665206),\n",
       " LinregressResult(slope=0.0079353513251476226, intercept=1.5654662020920358, rvalue=0.0013628641108414915, pvalue=0.89160828375528212, stderr=0.058231319952614241),\n",
       " LinregressResult(slope=0.038542785891290003, intercept=1.5592459780974448, rvalue=0.006635368419908934, pvalue=0.50703522250594468, stderr=0.058091412159082485),\n",
       " LinregressResult(slope=-0.051941502028888024, intercept=1.5774100461432101, rvalue=-0.0087813201889417672, pvalue=0.37992246044590161, stderr=0.059153627195025704),\n",
       " LinregressResult(slope=0.030594941731811619, intercept=1.5608665435215623, rvalue=0.0052194195006523636, pvalue=0.60175370469633516, stderr=0.058622582257933271),\n",
       " LinregressResult(slope=-0.012836578999945626, intercept=1.5696439274668681, rvalue=-0.0021888424437043496, pvalue=0.82676133094034598, stderr=0.058651238850074977),\n",
       " LinregressResult(slope=-0.0036084355962018627, intercept=1.5678521591526522, rvalue=-0.00062410473853527016, pvalue=0.95024216224560631, stderr=0.057823560185447556),\n",
       " LinregressResult(slope=0.040755012163731945, intercept=1.5589130946957686, rvalue=0.0069722703424235184, pvalue=0.48570995941153128, stderr=0.058457425717780405),\n",
       " LinregressResult(slope=-0.031465284540923615, intercept=1.5736559243820991, rvalue=-0.0054121321516730232, pvalue=0.58840452655659403, stderr=0.058143389301154785),\n",
       " LinregressResult(slope=-0.050163164559910482, intercept=1.576966930293157, rvalue=-0.0084593634748476344, pvalue=0.39763880956421327, stderr=0.059302791170537425)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
