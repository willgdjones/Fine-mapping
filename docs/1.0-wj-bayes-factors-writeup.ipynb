{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of $z$ values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we assume that the trait $y$ is modelled as:\n",
    "\n",
    "$$ y = X\\beta + \\epsilon $$\n",
    "\n",
    "Where $X$ is an $n$x$m$ matrix of values consisting of 0,1,2 denoting whether a SNP is homozygous to the common allele, heterozygous, or homozygous to the rare allele respectively. $n$ denotes the number of samples, and $m$ the number of causitive SNPs.\n",
    "\n",
    "We scale $X$ such that $\\frac{1}{n}\\sum^{n}_{i=1} X_{ij} = 0$, and $\\frac{1}{n}\\sum^{n}_{i=1} X^2_{ij} = 1$ for $j = 1,2, ... m$. We also scale $y$ such that $\\frac{1}{n}\\sum^n_{i=1} y_i = 0$ and $\\frac{1}{n}\\sum^n_{i=1} y_i^2 = 1$.\n",
    "\n",
    "We assume $\\epsilon$ ~ $N(0, \\frac{1}{\\tau} I_n)$. We also assume $\\beta$ has a prior normal distribution $N(0,\\nu \\frac{1}{\\tau})$. $\\nu$ is diagonal, $\\beta$ and $\\epsilon$ are independent and we assume all SNPs have the same prior variance $\\sigma^2 \\frac{1}{\\tau}$. Therefore $\\nu = \\sigma^2 I_m$.\n",
    "\n",
    "Now given this prior on $\\beta$, and using $X$ and $\\epsilon$, we can deduce the expectation and mean of $y$.\n",
    "\n",
    "$$E(y \\: | \\: \\tau, X) = E(E(y \\: | \\: \\tau,X,\\beta)) = E(X \\beta) = 0$$ \n",
    "\n",
    "<sub>[ *since* $E(\\beta) = 0$ ]</sub>\n",
    "\n",
    "$$ Var(y \\: | \\: \\tau, X) = E(Var(y \\: | \\tau, X, \\beta)) + Var(E(y \\: | \\: \\tau, X, \\beta)) $$\n",
    "\n",
    "<sub>[ *since* $Var(X \\: | \\: Y) = E(Var(X \\: | \\: Y)) + Var(E(X \\: | \\: Y))$ ]</sub>\n",
    "\n",
    "$$ = E(\\frac{1}{\\tau}I_n) + Var(X \\beta)$$\n",
    "\n",
    "$$ = \\frac{1}{\\tau}( I_n + X \\nu X^T)$$\n",
    "\n",
    "Now, since y is a linear transformation of a multivariate normal random vector,\n",
    "\n",
    "$$ y \\:|\\: \\tau, X \\sim N \\left( 0,\\frac{1}{\\tau}( I_n + X \\nu X^T)) \\right) $$\n",
    "\n",
    "The null distribution is when $\\beta = 0$. In which case,\n",
    "\n",
    "$$y \\:|\\: \\tau, X \\sim N \\left( 0,\\frac{1}{\\tau}I_n \\right) $$\n",
    "\n",
    "Now consider a new variable $z = \\sqrt{\\frac{\\tau}{n}} X^{T}y$:\n",
    "\n",
    "$$ z ~ \\sim N \\left( 0, \\frac{X^T}{n}(I_n + X \\nu X^T) X \\right)$$\n",
    "\n",
    "$$ = N \\left( 0, \\left(\\frac{X^TX}{n} + \\frac{X^TX \\nu X^TX}{n}\\right) \\right)$$\n",
    "\n",
    "Now let $\\Sigma_x = \\frac{X^T X}{n}$. Since all column in $X$ are standardised, this is equivalent to the correlation matrix or, more importantantly, the linkage disequilibirum structure of the SNPs which can be derived from the 1000 genomes data.\n",
    "\n",
    "Then we have:\n",
    "\n",
    "$$ z \\sim N(0, \\Sigma_x + \\Sigma_x n\\nu \\Sigma_x) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Bayes Factor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Bayes Factor* is the ratio of the likelihood functions under the alternative hypothesis, and under the null hypothesis. It is equivalent to the likelihood ratio.\n",
    "\n",
    "$P_1(z \\:|\\: \\tau, X)$, the likelihood of $z$ under our alternate hypothesis, i.e. when $\\nu \\neq 0$ is:\n",
    "\n",
    "$$ P_1(z \\:|\\: \\tau, X) = 2\\pi^{-\\frac{n}{2}} | \\Sigma_x + \\Sigma_x n\\nu \\Sigma_x |^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x + \\Sigma_x n\\nu \\Sigma_x)^{-1}z\\right)$$\n",
    "\n",
    "$P_0(z \\:| \\: \\tau, X)$, the likelihood of $z$ under the null hypothesis when $\\nu = 0$ is:\n",
    "\n",
    "$$P_0(z \\:| \\: \\tau, X) = 2\\pi^{-\\frac{n}{2}} |\\Sigma_x|^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x)^{-1}z\\right)$$\n",
    "\n",
    "Therefore we calculate the Bayes Factor as:\n",
    "\n",
    "$$ \\frac{\n",
    "| \\Sigma_x + n\\nu \\Sigma_x^2 |^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x + \\Sigma_x n\\nu \\Sigma_x)^{-1}z\\right)\n",
    "}{\n",
    "|\\Sigma_x|^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x)^{-1}z\\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "We assume that $X$ has full column rank, and that $\\Sigma_x$ also has rank $m$ and is non-singular. That is to say, we assume that no two snps are in full linkage disequilibrium.\n",
    "\n",
    "Using the Woodberry matrix identity:\n",
    "\n",
    "$$\n",
    "(\\Sigma_x + \\Sigma_x n\\nu \\Sigma_x)^{-1} = \\Sigma_x^{-1} - ((n\\nu)^{-1} + \\Sigma_x)^{-1}\n",
    "$$\n",
    "\n",
    "Therefore the resulting Bayes Factor is:\n",
    "\n",
    "$$\n",
    "|I_m + n\\nu \\Sigma_x|^\\frac{1}{2} \\exp(\\frac{1}{2}z^T((n\\nu)^{-1} + \\Sigma_x)^{-1}z)\n",
    "$$\n",
    "\n",
    "Crucially, this only depends on inverting matrices of size m, our candidate gene set. Therefore we compute these Bayes Factors using sets of candidate SNPs of size m, and choose the set with the highest calculated Bayes Factor.\n",
    "\n",
    "In practice, we recieve $\\beta$, $se(\\beta)$, and the SNP linkage disequilibrium structure $\\Sigma_x$.\n",
    "\n",
    "Since both $X$ and $y$ are normalised, \n",
    "\n",
    "$$\\beta = \\frac{X^T y}{n}$$\n",
    "\n",
    "Also, \n",
    "$$\\tau = \\frac{1}{\\sigma^2}, \\:\\: se(\\epsilon) = \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "where $\\sigma$ is the observed standard deviation of the errors $\\epsilon$.\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "se(\\epsilon) = \\frac{1}{\\sqrt{n\\tau}}\n",
    "$$\n",
    "\n",
    "Therefore we generate the $z$ vector exactly with and $se$ is the standard error:\n",
    "\n",
    "$$\n",
    "\\frac{\\beta}{se(\\beta)} = \\sqrt{\\frac{\\tau}{n}} X^{T}y = z\n",
    "$$\n",
    "\n",
    "The Bayes Factor can then be directly calculated using $z$ and $\\Sigma_x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We place a binomial prior on candidate gene sets. If our gene set $G$ has size $m$, we assume that each SNPs has probability  $p = \\frac{1}{m}$ of being causal. Therefore the prior probability of a causal gene set with size $l$ is:\n",
    "\n",
    "$$\n",
    "P(G) = p^l(1-p)^{m-l}\n",
    "$$\n",
    "\n",
    "Therefore using Bayes Theorem:\n",
    "\n",
    "$$\n",
    "P(G \\: | \\: X) = \\frac{P(X \\: | \\: G) \\times P(G)}{P(X)}\n",
    "$$\n",
    "\n",
    "to calculate posterior probabilities of the gene sets where $P(X \\: | \\: G)$ is calculated from the normalised Bayes Factors.\n",
    "\n",
    "However when we calculated the Bayes Factors, these are not exactly the likelihoods. They are however far easier to compute.\n",
    "\n",
    "The Bayes Factors we have calculated are equivalent to:\n",
    "\n",
    "$$\n",
    "\\frac{P(X \\: | \\: G)}{P(X \\: | \\: G_0)}\n",
    "$$\n",
    "\n",
    "where $G_0$ is the null hypothesis that no gene-set is casual.\n",
    "\n",
    "However, since $P(X \\: | \\: G_0)$ is a constant for all gene-sets, this is proportional to the likelihood term. Therefore we can normalise to output the posterior probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumerating all possible candidate SNP groups up to a given size is comprehensive, but scales impractically in both the maximum size of the gene set $K$, and the total candidate gene set size in the genotype array $m$. The search space scales $O(2^k)$, and for a fixed $k$, scales $O(m^k)$.\n",
    "\n",
    "At each iteration of the algorithm, stochastic search generates a neighbourhood of subsets $N$ around a specific subset $S$. This subset neighbourhood is generated by performing a single add, delete, or change operation to the original subset $S$, of size $k$. Creating an add-neighbourhood $A$, of size $m-k$ a delete-neighbourhood $D$, of size $k$, and a change-neighbourhood $C$, of size $k(m-k)$. The total neighbourhood therefore is:\n",
    "\n",
    "$$\n",
    "N = A \\cup D \\cup C\n",
    "$$\n",
    "\n",
    "Then, we computed bayes factors of each set in this neighbourhood and store the results in a hash map. Subset scores are consistent across iterations, therefore if already traversed they can be looked up in constant time to avoid them being expensively recomputed.\n",
    "\n",
    "The set with the largest bayes factor is returned, and is used as the input to the next iteration. The algorithm terminates when it produced a loop, or when it reaches the maximum number of iterations.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
