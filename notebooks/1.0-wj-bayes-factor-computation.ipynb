{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "### Load modules and data\n",
    "\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy import stats\n",
    "import pdb\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "from unittest import *\n",
    "import itertools\n",
    "from bidict import bidict\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single trait Fine-mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Factor Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of $z$ values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we assume that the trait $y$ is modelled as:\n",
    "\n",
    "$$ y = X\\beta + \\epsilon $$\n",
    "\n",
    "Where $X$ is an $n$x$m$ matrix of values consisting of 0,1,2 denoting whether a SNP is homozygous to the common allele, heterozygous, or homozygous to the rare allele respectively. $n$ denotes the number of samples, and $m$ the number of causitive SNPs.\n",
    "\n",
    "We scale $X$ such that $\\frac{1}{n}\\sum^{n}_{i=1} X_{ij} = 0$, and $\\frac{1}{n}\\sum^{n}_{i=1} X^2_{ij} = 1$ for $j = 1,2, ... m$. We also scale $y$ such that $\\frac{1}{n}\\sum^n_{i=1} y_i = 0$ and $\\frac{1}{n}\\sum^n_{i=1} y_i^2 = 1$.\n",
    "\n",
    "We assume $\\epsilon$ ~ $N(0, \\frac{1}{\\tau} I_n)$. We also assume $\\beta$ has a prior normal distribution $N(0,\\nu \\frac{1}{\\tau})$. $\\nu$ is diagonal, $\\beta$ and $\\epsilon$ are independent and we assume all SNPs have the same prior variance $\\sigma^2 \\frac{1}{\\tau}$. Therefore $\\nu = \\sigma^2 I_m$.\n",
    "\n",
    "Now given this prior on $\\beta$, and using $X$ and $\\epsilon$, we can deduce the expectation and mean of $y$.\n",
    "\n",
    "$$E(y \\: | \\: \\tau, X) = E(E(y \\: | \\: \\tau,X,\\beta)) = E(X \\beta) = 0$$ \n",
    "\n",
    "<sub>[ *since* $E(\\beta) = 0$ ]</sub>\n",
    "\n",
    "$$ Var(y \\: | \\: \\tau, X) = E(Var(y \\: | \\tau, X, \\beta)) + Var(E(y \\: | \\: \\tau, X, \\beta)) $$\n",
    "\n",
    "<sub>[ *since* $Var(X \\: | \\: Y) = E(Var(X \\: | \\: Y)) + Var(E(X \\: | \\: Y))$ ]</sub>\n",
    "\n",
    "$$ = E(\\frac{1}{\\tau}I_n) + Var(X \\beta)$$\n",
    "\n",
    "$$ = \\frac{1}{\\tau}( I_n + X \\nu X^T)$$\n",
    "\n",
    "Now, since y is a linear transformation of a multivariate normal random vector,\n",
    "\n",
    "$$ y \\:|\\: \\tau, X \\sim N \\left( 0,\\frac{1}{\\tau}( I_n + X \\nu X^T)) \\right) $$\n",
    "\n",
    "The null distribution is when $\\beta = 0$. In which case,\n",
    "\n",
    "$$y \\:|\\: \\tau, X \\sim N \\left( 0,\\frac{1}{\\tau}I_n \\right) $$\n",
    "\n",
    "Now consider a new variable $z = \\sqrt{\\frac{\\tau}{n}} X^{T}y$:\n",
    "\n",
    "$$ z ~ \\sim N \\left( 0, \\frac{X^T}{n}(I_n + X \\nu X^T) X \\right)$$\n",
    "\n",
    "$$ = N \\left( 0, \\left(\\frac{X^TX}{n} + \\frac{X^TX \\nu X^TX}{n}\\right) \\right)$$\n",
    "\n",
    "Now let $\\Sigma_x = \\frac{X^T X}{n}$. Since all column in $X$ are standardised, this is equivalent to the correlation matrix or, more importantantly, the linkage disequilibirum structure of the SNPs which can be derived from the 1000 genomes data.\n",
    "\n",
    "Then we have:\n",
    "\n",
    "$$ z \\sim N(0, \\Sigma_x + \\Sigma_x n\\nu \\Sigma_x) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Bayes Factor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Bayes Factor* is the ratio of the likelihood functions under the alternative hypothesis, and under the null hypothesis. It is equivalent to the likelihood ratio.\n",
    "\n",
    "$P_1(z \\:|\\: \\tau, X)$, the likelihood of $z$ under our alternate hypothesis, i.e. when $\\nu \\neq 0$ is:\n",
    "\n",
    "$$ P_1(z \\:|\\: \\tau, X) = 2\\pi^{-\\frac{n}{2}} | \\Sigma_x + \\Sigma_x n\\nu \\Sigma_x |^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x + \\Sigma_x n\\nu \\Sigma_x)^{-1}z\\right)$$\n",
    "\n",
    "$P_0(z \\:| \\: \\tau, X)$, the likelihood of $z$ under the null hypothesis when $\\nu = 0$ is:\n",
    "\n",
    "$$P_0(z \\:| \\: \\tau, X) = 2\\pi^{-\\frac{n}{2}} |\\Sigma_x|^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x)^{-1}z\\right)$$\n",
    "\n",
    "Therefore we calculate the Bayes Factor as:\n",
    "\n",
    "$$ \\frac{\n",
    "| \\Sigma_x + n\\nu \\Sigma_x^2 |^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x + \\Sigma_x n\\nu \\Sigma_x)^{-1}z\\right)\n",
    "}{\n",
    "|\\Sigma_x|^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}z^T(\\Sigma_x)^{-1}z\\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "We assume that $X$ has full column rank, and that $\\Sigma_x$ also has rank $m$ and is non-singular. That is to say, we assume that no two snps are in full linkage disequilibrium.\n",
    "\n",
    "Using the Woodberry matrix identity:\n",
    "\n",
    "$$\n",
    "(\\Sigma_x + \\Sigma_x n\\nu \\Sigma_x)^{-1} = \\Sigma_x^{-1} - ((n\\nu)^{-1} + \\Sigma_x)^{-1}\n",
    "$$\n",
    "\n",
    "Therefore the resulting Bayes Factor is:\n",
    "\n",
    "$$\n",
    "|I_m + n\\nu \\Sigma_x|^\\frac{1}{2} \\exp(\\frac{1}{2}z^T((n\\nu)^{-1} + \\Sigma_x)^{-1}z)\n",
    "$$\n",
    "\n",
    "Crucially, this only depends on inverting matrices of size m, our candidate gene set. Therefore we compute these Bayes Factors using sets of candidate SNPs of size m, and choose the set with the highest calculated Bayes Factor.\n",
    "\n",
    "In practice, we recieve $\\beta$, $se(\\beta)$, and the SNP linkage disequilibrium structure $\\Sigma_x$.\n",
    "\n",
    "Since both $X$ and $y$ are normalised, \n",
    "\n",
    "$$\\beta = \\frac{X^T y}{n}$$\n",
    "\n",
    "Also, \n",
    "$$\\tau = \\frac{1}{\\sigma^2}, \\:\\: se(\\epsilon) = \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "where $\\sigma$ is the observed standard deviation of the errors $\\epsilon$.\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "se(\\epsilon) = \\frac{1}{\\sqrt{n\\tau}}\n",
    "$$\n",
    "\n",
    "Therefore we generate the $z$ vector exactly with and $se$ is the standard error:\n",
    "\n",
    "$$\n",
    "\\frac{\\beta}{se(\\beta)} = \\sqrt{\\frac{\\tau}{n}} X^{T}y = z\n",
    "$$\n",
    "\n",
    "The Bayes Factor can then be directly calculated using $z$ and $\\Sigma_x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We place a binomial prior on candidate gene sets. If our gene set $G$ has size $m$, we assume that each SNPs has probability  $p = \\frac{1}{m}$ of being causal. Therefore the prior probability of a causal gene set with size $l$ is:\n",
    "\n",
    "$$\n",
    "P(G) = p^l(1-p)^{m-l}\n",
    "$$\n",
    "\n",
    "Therefore using Bayes Theorem:\n",
    "\n",
    "$$\n",
    "P(G \\: | \\: X) = \\frac{P(X \\: | \\: G) \\times P(G)}{P(X)}\n",
    "$$\n",
    "\n",
    "to calculate posterior probabilities of the gene sets where $P(X \\: | \\: G)$ is calculated from the normalised Bayes Factors.\n",
    "\n",
    "However when we calculated the Bayes Factors, these are not exactly the likelihoods. They are however far easier to compute.\n",
    "\n",
    "The Bayes Factors we have calculated are equivalent to:\n",
    "\n",
    "$$\n",
    "\\frac{P(X \\: | \\: G)}{P(X \\: | \\: G_0)}\n",
    "$$\n",
    "\n",
    "where $G_0$ is the null hypothesis that no gene-set is casual.\n",
    "\n",
    "However, since $P(X \\: | \\: G_0)$ is a constant for all gene-sets, this is proportional to the likelihood term. Therefore we can normalise to output the posterior probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "### Create selection of SNPs\n",
    "def select_snps(z, subset):\n",
    "    return [z[i] for i in subset]\n",
    "\n",
    "#example\n",
    "# for subset in it.combinations(range(len(z1)),3):\n",
    "#     print subset, select_snps(z1, subset)    \n",
    "\n",
    "\n",
    "\n",
    "### Select covariance submatrix\n",
    "\n",
    "def select_cov(cov, subset):\n",
    "    return cov[np.ix_(subset,subset)]\n",
    "\n",
    "#example   \n",
    "#select_cov(LD_tss_1, (0,1,5))\n",
    "\n",
    "### Calculate Bayes Factor\n",
    "\n",
    "def calc_BF(z, cov,n,v=0.1):\n",
    "    \"\"\"\n",
    "    Calculate the Bayes factor of a single set of candidate SNPs effect sizes z,\n",
    "    covariance matrix cov, a prior variance on beta v, and a sample\n",
    "    size n.\n",
    "    \"\"\"\n",
    "    z = np.matrix(z)\n",
    "    z = z.T\n",
    "    v_matrix = np.matrix(np.eye(len(z)) * v)\n",
    "#     pdb.set_trace()\n",
    "    coeff = 1. / math.sqrt(np.linalg.det((np.matrix(np.eye(len(z))) + n * v_matrix * np.matrix(cov))))\n",
    "    exponent = 0.5* z.T * np.matrix(np.linalg.pinv((n*v_matrix).I + cov)) * z\n",
    "    return np.array(math.log(coeff) + exponent)[0][0]\n",
    "\n",
    "# example\n",
    "# subset = (0,1,5,8)\n",
    "# cov = select_cov(LD_tss_1, subset)\n",
    "# z = select_snps(z1, subset)\n",
    "# v = np.eye(len(z))/1000\n",
    "# n = 1000\n",
    "# calc_BF(z,cov,v,n)\n",
    "\n",
    "def calc_prior(x,m,prior='binomial'):\n",
    "    if prior == 'binomial':\n",
    "        p = 1./m\n",
    "        l = len(x)\n",
    "        return p**l * (1-p)**(m-l)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# example\n",
    "# calc_prior((1,3,5),30)\n",
    "    \n",
    "def calc_posterior(variant_set_BF,prior='binomial'):\n",
    "    \n",
    "    priors = [math.log(calc_prior(x[0],30)) for x in variant_set_BF]\n",
    "    \n",
    "    log_bayes_factors = [x[1] for x in variant_set_BF]\n",
    "\n",
    "    unscaled_log_posteriors = [ log_bayes_factors[i] + priors[i] for i in range(len(log_bayes_factors))]\n",
    "\n",
    "    scaled_log_posteriors = np.array(unscaled_log_posteriors) - max(unscaled_log_posteriors)\n",
    "\n",
    "    scaled_posteriors = [math.exp(x) for x in scaled_log_posteriors]\n",
    "\n",
    "    calib_factor = sum([math.exp(x) for x in scaled_log_posteriors])\n",
    "\n",
    "    posteriors = [x/calib_factor for x in [math.exp(x) for x in scaled_log_posteriors]]\n",
    "    \n",
    "    aug_posteriors = [(variant_set_BF[i][0], posteriors[i]) for i in range(len(posteriors))]\n",
    "    \n",
    "    aug_posteriors.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return aug_posteriors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_variant_set_BFs(data,k,v=0.1,prior='binomial'):\n",
    "    \"\"\"\n",
    "    Calculate variant set posteriors with a binomial prior as normal,\n",
    "    searching all variant sets up till size k.\n",
    "    v is the prior variance on beta.\n",
    "    data has the format (z,LD,n) where z is the effect sizes, \n",
    "    LD is the linkage disequilibrium matrix, and n is the \n",
    "    number of samples.\n",
    "    \"\"\"\n",
    "    bayes_factors = []\n",
    "    for i in range(1,k):\n",
    "        for subset in it.combinations(range(len(data[0])),i):\n",
    "            z = select_snps(data[0], subset)\n",
    "            cov = select_cov(data[1],subset)\n",
    "            n = data[2]\n",
    "            bayes_factors.append((subset, calc_BF(z, cov,n,v)))\n",
    "    \n",
    "    bayes_factors.sort(key=lambda x: x[1], reverse=True)\n",
    "    return bayes_factors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_tss_1=np.load('../data/raw/summary_stats_g1_tss60.npy')[0]\n",
    "s_tss_2=np.load('../data/raw/summary_stats_g2_tss60.npy')[0]\n",
    "LD_tss_1=np.load('../data/raw/LD_g1_TSS60.npy')\n",
    "LD_tss_2=np.load('../data/raw/LD_g2_TSS60.npy')\n",
    "\n",
    "### Generate z arrays\n",
    "\n",
    "n1 = 10000\n",
    "n2 = 1000\n",
    "z1 = np.array(np.divide(s_tss_1['beta'],np.sqrt(s_tss_1['var_beta'])))\n",
    "z2 = np.array(np.divide(s_tss_2['beta'],np.sqrt(s_tss_1['var_beta'])))\n",
    "z1 = np.ndarray.flatten(z1)\n",
    "z2 = np.ndarray.flatten(z2)\n",
    "\n",
    "### Initialise hyper parameters\n",
    "k=3\n",
    "data1 = (z1, LD_tss_1, 10000)\n",
    "data2 = (z2, LD_tss_2, 1000)\n",
    "\n",
    "### Calculate variant set Bayes Factors\n",
    "set1 = calc_variant_set_BFs(data1,k)\n",
    "set2 = calc_variant_set_BFs(data2,k)\n",
    "\n",
    "### Calculate variant set posteriors\n",
    "posteriors1 = calc_posterior(set1)\n",
    "posteriors2 = calc_posterior(set2)\n",
    "\n",
    "posteriors1.sort(key=lambda x: x[1], reverse=True)\n",
    "posteriors2.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "posteriors2[0:10]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
